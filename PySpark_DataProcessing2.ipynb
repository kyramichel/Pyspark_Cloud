{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PySpark_DataProcessing2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXdXdKt7bJH"
      },
      "source": [
        "#  Big Data Processing with Spark\r\n",
        "\r\n",
        "In my earlier 3 notebooks, I discussed in detail about how to install Spark fast, uploading data in Colab, dealing with missing data values and selecting fields. \r\n",
        "\r\n",
        "This notebook focuses on Data Processing and Data Analysis as a way to get useful information from data\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib7UCnGX6-z2"
      },
      "source": [
        "## PART 1. Configure PySpark environment\n",
        "\n",
        "Copy & Paste code below. \n",
        "\n",
        "Read more https://github.com/kyramichel/Spark/blob/master/PySpark_GoogleColab.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdSQHcj0OI6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f195ed0-a65b-4658-cdf8-cfed58ca28f2"
      },
      "source": [
        "#update the packages existing on the machine\n",
        "!apt-get update\n",
        "\n",
        "#install java \n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "\n",
        "#install spark: get the file\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
        "    \n",
        "#unzip the file\n",
        "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
        "\n",
        "#set up the ennvironmental variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.2-bin-hadoop2.7\"\n",
        "\n",
        "#install finspark  \n",
        "!pip install -q findspark\n",
        "\n",
        "#importing findspark adds pyspark to the system path, so that next time you can import pyspark like any other python library\n",
        "import findspark\n",
        "findspark.init(\"/content/spark-2.4.1-bin-hadoop2.7\")\n",
        "\n",
        "import pyspark\n",
        "\n",
        "#SparkContext: the entry point of spark functionality is the interface to running a spark cluster manager\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "\n",
        "#import a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "#create a session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark\n",
        "\n",
        "#test the installation\n",
        "df0 = spark.sql(\"select 'PySpark' as Hello\")\n",
        "df0.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 43.1 kB/88.7\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [5 InRelease 14.2 kB/15.9 k\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [5 InRelease 14.2 kB/15.9 k\r                                                                               \rGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [5 InRelease 14.2 kB/15.9 k\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [5 InRelease 14.2 kB/15.9 k\r                                                                               \rGet:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,964 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,396 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [602 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,746 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,394 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [893 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,163 kB]\n",
            "Fetched 11.4 MB in 2s (4,594 kB/s)\n",
            "Reading package lists... Done\n",
            "+-------+\n",
            "|  Hello|\n",
            "+-------+\n",
            "|PySpark|\n",
            "+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP2pY0E03QYX"
      },
      "source": [
        "# PART 2. Data Processing\r\n",
        "\r\n",
        "Let's get deeper in data processing and query data in Spark using both Python and SQL.\r\n",
        "\r\n",
        "First, click on File panel, then upload and select your data file to upload it in Colab. More on how to get data in Colab @ https://github.com/kyramichel/Spark/blob/master/DataPysparkCloudColab.ipynb\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6cOc3XiLjnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1a1134-5042-4e82-f5e3-e4d67dc0ac26"
      },
      "source": [
        "#loadd data as df\n",
        "df = spark.read.csv('data2.csv', header=True, inferSchema=True)\n",
        "df.show(5)  "
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+-----+-----------------+--------------+-----+-------------+--------+---------+------+\n",
            "| id| Product|Price|             Name|          City|State|      Country|Latitude|Longitude|US Zip|\n",
            "+---+--------+-----+-----------------+--------------+-----+-------------+--------+---------+------+\n",
            "|  1|Product1| 1200|           Betina|     Parkville|   MO|United States|  39.195|-94.68194| 64152|\n",
            "|  2|Product1| 1200|Federica e Andrea|       Astoria|   OR|United States|46.18806|  -123.83| 97103|\n",
            "|  3|Product2| 3600|           Gerd W|Cahaba Heights|   AL|United States|33.52056| -86.8025| 35243|\n",
            "|  4|Product1| 1200|         LAURENCE|     Mickleton|   NJ|United States|   39.79|-75.23806|  8056|\n",
            "|  5|Product1| 1200|            Fleur|        Peoria|   IL|United States|40.69361|-89.58889| 61601|\n",
            "+---+--------+-----+-----------------+--------------+-----+-------------+--------+---------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6DFTHR-Hhll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef186ca-755c-48ef-a42c-1d6ac1699480"
      },
      "source": [
        "#check data types\r\n",
        "df.dtypes"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', 'int'),\n",
              " ('Product', 'string'),\n",
              " ('Price', 'int'),\n",
              " ('Name', 'string'),\n",
              " ('City', 'string'),\n",
              " ('State', 'string'),\n",
              " ('Country', 'string'),\n",
              " ('Latitude', 'double'),\n",
              " ('Longitude', 'double'),\n",
              " ('US Zip', 'int')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PoEpHAXBp8X"
      },
      "source": [
        "## Eliminat uneeded features & missing values using drop():\r\n",
        "\r\n",
        "More details on drop() and select() at https://github.com/kyramichel/Spark/blob/master/PySpark_DataProcessing1.ipynb\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr_sFgKHC_AB"
      },
      "source": [
        "#Eliminate undeeded columns\r\n",
        "col_list = ['Name', 'City', \"US Zip\"]\r\n",
        "df1= df.drop(*col_list)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqKDu2KVMa20",
        "outputId": "00e86097-e240-4a5a-a4b8-46c2b60c7dba"
      },
      "source": [
        "#check columns\r\n",
        "df1.columns"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id', 'Product', 'Price', 'State', 'Country', 'Latitude', 'Longitude']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndxFNGSTKNFX"
      },
      "source": [
        "#Eliminate all rows that contain missing values\r\n",
        "df2 = df1.na.drop()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSy77HCkPFjT",
        "outputId": "847b4c78-a2b5-4e95-e980-41743313f6cc"
      },
      "source": [
        "#return the number of missing values per column\r\n",
        "from pyspark.sql.functions import col, when, count\r\n",
        "df2.select(*(count(when(col(c).isNull(), c)).alias(c) for c in df2.columns)).show()\r\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+-----+-----+-------+--------+---------+\n",
            "| id|Product|Price|State|Country|Latitude|Longitude|\n",
            "+---+-------+-----+-----+-------+--------+---------+\n",
            "|  0|      0|    0|    0|      0|       0|        0|\n",
            "+---+-------+-----+-----+-------+--------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXgN6NW4MxOU",
        "outputId": "68973ffd-628f-42d6-b02e-fed4b293cf5e"
      },
      "source": [
        "df2.show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+-----+-----+-------------+--------+----------+\n",
            "| id| Product|Price|State|      Country|Latitude| Longitude|\n",
            "+---+--------+-----+-----+-------------+--------+----------+\n",
            "|  1|Product1| 1200|   MO|United States|  39.195| -94.68194|\n",
            "|  2|Product1| 1200|   OR|United States|46.18806|   -123.83|\n",
            "|  3|Product2| 3600|   AL|United States|33.52056|  -86.8025|\n",
            "|  4|Product1| 1200|   NJ|United States|   39.79| -75.23806|\n",
            "|  5|Product1| 1200|   IL|United States|40.69361| -89.58889|\n",
            "|  6|Product1| 1200|   TN|United States|36.34333| -88.85028|\n",
            "|  7|Product1| 1200|   NY|United States|40.71417| -74.00639|\n",
            "|  8|Product1| 1200|   TX|United States|29.42389| -98.49333|\n",
            "|  9|Product1| 1200|   ID|United States|43.69556|-116.35306|\n",
            "| 10|Product1| 1200|   NJ|United States|40.03222| -74.95778|\n",
            "| 11|Product1| 1200|   UT|United States|40.76083|-111.89028|\n",
            "| 12|Product1| 1200|   CA|United States|   32.64|-117.08333|\n",
            "| 13|Product1| 1200|   TX|United States|29.61944| -95.63472|\n",
            "| 14|Product1| 1200|   NY|United States|40.71417| -74.00639|\n",
            "| 15|Product1| 1200|   IL|United States|40.61278| -89.45917|\n",
            "| 16|Product1| 1200|   CA|United States|37.22667|-121.97361|\n",
            "| 17|Product1| 1200|   NY|United States|40.71417| -74.00639|\n",
            "| 18|Product1| 1200|   FL|United States|25.77389| -80.19389|\n",
            "| 19|Product1| 1200|   NY|United States|   40.65|    -73.95|\n",
            "| 20|Product1| 1200|   VA|United States|38.96861| -77.34139|\n",
            "+---+--------+-----+-----+-------------+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri3sCw9iL1_Y"
      },
      "source": [
        "### Describe data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYq1NaZwKYbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d39b263-0fa8-409e-ac4c-67de4b06dd7a"
      },
      "source": [
        "#summary statistics - mean, std for numeric fields\r\n",
        "df2.describe().show()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+--------+------------------+------+-------------+------------------+-------------------+\n",
            "|summary|                id| Product|             Price| State|      Country|          Latitude|          Longitude|\n",
            "+-------+------------------+--------+------------------+------+-------------+------------------+-------------------+\n",
            "|  count|               989|     989|               989|   989|          989|               989|                989|\n",
            "|   mean| 497.3346814964611|    null|1635.2881698685542|  null|         null| 39.02358567118307|-41.816108273407494|\n",
            "| stddev|288.60640692444576|    null|1158.9440271070878|  null|         null|19.557241666334374|  67.34419133543398|\n",
            "|    min|                 1|Product1|               250|    AK|    Argentina|           -41.465|         -159.48528|\n",
            "|    max|               998|Product3|             13000|Zurich|United States|          64.83778|        174.7666667|\n",
            "+-------+------------------+--------+------------------+------+-------------+------------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lBXNJImSbKE"
      },
      "source": [
        "## Interpolation using ML\r\n",
        "\r\n",
        "Interpolation allows to imput missing values using other records in the dataset. More details: https://github.com/kyramichel/Spark/blob/master/Data_Cleansing_Pyspark.ipynb\r\n",
        "\r\n",
        "Here I use Spark ML to simply impute the NaN by calling an Imputer:\r\n",
        "\r\n",
        "Imputation strategyies: \"mean\", \"median\" and \"mode\"\r\n",
        "\r\n",
        "- mean imputation strategy replaces missing values using the mean value of the column\r\n",
        "\r\n",
        "- median and  mode are useful for imputing categorical feature values.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQVTaWWmSs4I"
      },
      "source": [
        "#mean imputation \r\n",
        "from pyspark.ml.feature import Imputer\r\n",
        "imputer = Imputer(inputCols=[\"Latitude\"],outputCols=[\"Latitude\"])\r\n",
        "model = imputer.fit(df2)\r\n",
        "df2 = model.transform(df2)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-kpuWOImk6T"
      },
      "source": [
        "#median imputation\r\n",
        "from pyspark.ml.feature import Imputer\r\n",
        "imputer = Imputer(inputCols=[\"Longitude\"],outputCols=[\"Longitude\"]).setStrategy(\"median\")\r\n",
        "model = imputer.fit(df2)\r\n",
        "df2 = model.transform(df2)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyH7F-dpm7e4",
        "outputId": "d45f70a1-f3dc-479d-8b0c-2f42f4dfb8ca"
      },
      "source": [
        "df2.show(3)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+-----+-----+-------------+--------+---------+\n",
            "| id| Product|Price|State|      Country|Latitude|Longitude|\n",
            "+---+--------+-----+-----+-------------+--------+---------+\n",
            "|  1|Product1| 1200|   MO|United States|  39.195|-94.68194|\n",
            "|  2|Product1| 1200|   OR|United States|46.18806|  -123.83|\n",
            "|  3|Product2| 3600|   AL|United States|33.52056| -86.8025|\n",
            "+---+--------+-----+-----+-------------+--------+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T99M36cInSBS",
        "outputId": "890ec807-3bde-417d-f945-6bf4988f2f98"
      },
      "source": [
        "#Imputer needs col to be imputed be float/double otherwise it will throw a casting error\r\n",
        "df2.dtypes"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', 'int'),\n",
              " ('Product', 'string'),\n",
              " ('Price', 'int'),\n",
              " ('State', 'string'),\n",
              " ('Country', 'string'),\n",
              " ('Latitude', 'double'),\n",
              " ('Longitude', 'double')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYMG4peVj56G",
        "outputId": "f56b516e-b4e3-4015-ee7f-5a2279f50ba7"
      },
      "source": [
        "#first cast Price to float\r\n",
        "df3 = df2.withColumn(\"Price\", df2['Price'].cast('float'))\r\n",
        "df3.dtypes"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', 'int'),\n",
              " ('Product', 'string'),\n",
              " ('Price', 'float'),\n",
              " ('State', 'string'),\n",
              " ('Country', 'string'),\n",
              " ('Latitude', 'double'),\n",
              " ('Longitude', 'double')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuu1epC3h0ot"
      },
      "source": [
        "#mode imputation\r\n",
        "imputer = Imputer(inputCols=['Price'],outputCols=['Price']).setStrategy(\"mode\")\r\n",
        "model = imp.fit(df3)\r\n",
        "df3 =model.transform(df3)\r\n"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbGNHKvDq5Hb"
      },
      "source": [
        "imputer = Imputer(inputCols=[\"Price\", \"Latitude\",\"Longitude\"],outputCols=[\"Price\", \"Latitude\",\"Longitude\"])\r\n",
        "model = imputer.fit(df3)\r\n",
        "df3 = model.transform(df3)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLafjXNwgBDN"
      },
      "source": [
        "\r\n",
        "*In general, you can use Imputer for all columns/slice = df.columns[a:b] of a df:*\r\n",
        "\r\n",
        "\r\n",
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "df = df.select(*(col(c).cast(\"float\").alias(c) for c in df.columns))\r\n",
        "\r\n",
        "imputer = Imputer(inputCols=df.columns, outputCols=[\"{}_cleaned\".format(c) for c in df.columns])\r\n",
        "\r\n",
        "\r\n",
        "etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyDC4ZBZuUdJ"
      },
      "source": [
        "## Creating a features column\r\n",
        "\r\n",
        "In ML is useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky-Net52vJ3D",
        "outputId": "c9a87152-2ef7-4742-cb91-c2960b782777"
      },
      "source": [
        "feature_cols = ['Latitude', 'Longitude'] #omit undeeded columns\r\n",
        "print(feature_cols)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Latitude', 'Longitude']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHimFCM9vjMO"
      },
      "source": [
        "#import the vector assembler\r\n",
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYQJc0ELuMtq"
      },
      "source": [
        "assembler = VectorAssembler(inputCols=feature_cols,outputCol=\"geo_coor\")\r\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C17bs_WqwesV",
        "outputId": "44c277d3-ae40-4438-cd1c-88fa22b71b40"
      },
      "source": [
        "#use transform method to transform our dataset\r\n",
        "df4 = assembler.transform(df3)  \r\n",
        "df4.show(5)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+------+-----+-------------+--------+---------+--------------------+\n",
            "| id| Product| Price|State|      Country|Latitude|Longitude|            geo_coor|\n",
            "+---+--------+------+-----+-------------+--------+---------+--------------------+\n",
            "|  1|Product1|1200.0|   MO|United States|  39.195|-94.68194|  [39.195,-94.68194]|\n",
            "|  2|Product1|1200.0|   OR|United States|46.18806|  -123.83|  [46.18806,-123.83]|\n",
            "|  3|Product2|3600.0|   AL|United States|33.52056| -86.8025| [33.52056,-86.8025]|\n",
            "|  4|Product1|1200.0|   NJ|United States|   39.79|-75.23806|   [39.79,-75.23806]|\n",
            "|  5|Product1|1200.0|   IL|United States|40.69361|-89.58889|[40.69361,-89.58889]|\n",
            "+---+--------+------+-----+-------------+--------+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg8XirLHzRtA"
      },
      "source": [
        "### Feature scaling is an important step in ML data preprocessing \r\n",
        "\r\n",
        "We can use StandardScaler to scalerize the “feature” column\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uOuIu2TzRCn"
      },
      "source": [
        "from pyspark.ml.feature import StandardScaler\r\n"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYnOz4Ya0Yrn"
      },
      "source": [
        "standardscaler=StandardScaler().setInputCol(\"geo_coor\").setOutputCol(\r\n",
        "\"Scaled_coor\")\r\n",
        "\r\n",
        "df5=standardscaler.fit(df4).transform(df4)\r\n"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHgi4EQu0vb5",
        "outputId": "c896429c-73f6-4df5-a5a5-97cfca270336"
      },
      "source": [
        "df5.show(3)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+------+-----+-------------+--------+---------+-------------------+--------------------+\n",
            "| id| Product| Price|State|      Country|Latitude|Longitude|           geo_coor|         Scaled_coor|\n",
            "+---+--------+------+-----+-------------+--------+---------+-------------------+--------------------+\n",
            "|  1|Product1|1200.0|   MO|United States|  39.195|-94.68194| [39.195,-94.68194]|[2.00411697460740...|\n",
            "|  2|Product1|1200.0|   OR|United States|46.18806|  -123.83| [46.18806,-123.83]|[2.36168580355110...|\n",
            "|  3|Product2|3600.0|   AL|United States|33.52056| -86.8025|[33.52056,-86.8025]|[1.71397176411139...|\n",
            "+---+--------+------+-----+-------------+--------+---------+-------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlqvN289yFju"
      },
      "source": [
        "## Partitioning a dataset: Train-Test Split \r\n",
        "\r\n",
        "is data preprocessing technique needed in ML \r\n",
        "\r\n",
        "Ex: 70-30 train-test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIsugn4QyEjf"
      },
      "source": [
        "train, test = df4.randomSplit([0.7, 0.3], seed=123)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_PqAp5q1aK8",
        "outputId": "848e3902-dba5-4f5c-c9da-62609bf353b0"
      },
      "source": [
        "df5.count()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "989"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot8e234-1QcR",
        "outputId": "2e01d509-3a3c-477e-b374-68dda682fbb7"
      },
      "source": [
        "train.count()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "701"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR8J2uYo1Wee",
        "outputId": "392e3e7e-34ff-4643-fc84-ce1829dcb835"
      },
      "source": [
        "test.count()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    }
  ]
}