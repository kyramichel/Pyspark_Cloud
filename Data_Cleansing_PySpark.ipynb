{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pyspark_Cloud v2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXdXdKt7bJH"
      },
      "source": [
        "# PySpark: Cleaning Data and Getting insights from data\r\n",
        "\r\n",
        "\r\n",
        "- There are 3 Parts: Installing Spark, Loading & Data cleaning and Getting insights from data. These are the entry points for any data analytics project \r\n",
        "\r\n",
        "- In my earlier notebooks, I discussed in detail about Spark installationa and uploading data in Colab. This notebook is focused on Data cleansing. \r\n",
        "\r\n",
        "- Data cleansing is the process of analyzing the quality of data in a data source, approving/rejecting the suggestions by the system and making changes to the data.The quality of data is important in getting useful information from it.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib7UCnGX6-z2"
      },
      "source": [
        "## PART 1. Configure PySpark environment\n",
        "\n",
        "Copy & Paste code below. \n",
        "\n",
        "Read more https://github.com/kyramichel/Pyspark_Cloud/blob/master/PySpark_GoogleColab.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdSQHcj0OI6Q"
      },
      "source": [
        "#update the packages existing on the machine\n",
        "!apt-get update\n",
        "\n",
        "#install java \n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "\n",
        "#install spark: get the file\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
        "    \n",
        "#unzip the file\n",
        "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
        "\n",
        "#set up the ennvironmental variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.2-bin-hadoop2.7\"\n",
        "\n",
        "#install finspark  \n",
        "!pip install -q findspark\n",
        "\n",
        "#importing findspark adds pyspark to the system path, so that next time you can import pyspark like any other python library\n",
        "import findspark\n",
        "findspark.init(\"/content/spark-2.4.1-bin-hadoop2.7\")\n",
        "\n",
        "import pyspark\n",
        "\n",
        "#SparkContext: the entry point of spark functionality is the interface to running a spark cluster manager\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "\n",
        "#import a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "#create a session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark\n",
        "\n",
        "#test the installation\n",
        "df0 = spark.sql(\"select 'PySpark' as Hello\")\n",
        "df0.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP2pY0E03QYX"
      },
      "source": [
        "# PART 2. Upload, Load & Clean Data\r\n",
        "\r\n",
        "- To upload data, click upload, select your data file\r\n",
        "Read more how to get in data in Colab: https://github.com/kyramichel/Pyspark_Cloud/blob/master/DataPysparkCloudColab.ipynb\r\n",
        "\r\n",
        "\r\n",
        "- Load data, create a data frame df \r\n",
        "\r\n",
        "- To get insights from data we can query a data frame in Spark using both Python and SQL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6cOc3XiLjnN"
      },
      "source": [
        "df = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
        "df.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6DFTHR-Hhll"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKphjZjHOlC-"
      },
      "source": [
        "#Import pyspark sql functions library to clean data\n",
        "from pyspark.sql.functions import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7nz-ycPOFDH"
      },
      "source": [
        "#clean Region column - we create a new col because df is immutable\n",
        "df1 = df.withColumn(\"RegionCleaned\", when(df.Region.isNull(), 'unknown').otherwise(df.Region))\n",
        "df1.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ith704sXQkfh"
      },
      "source": [
        "df1.select(\"Region\", \"RegionCleaned\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUyNWXT1lJQC"
      },
      "source": [
        "df1.drop(\"Region\")\r\n",
        "df1.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pjaI47zlR45"
      },
      "source": [
        "df1 = df1.withColumnRenamed(\"RegionCleaned\",\"Region\")\r\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax2R0LrySfUU"
      },
      "source": [
        "#Use filter to delete entire row when Country is Null  \n",
        "\n",
        "df1 = df.filter(df.Country.isNotNull())\n",
        "df1.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vir-lsKp_Tfu"
      },
      "source": [
        "df2 = df1.withColumn(\"PriceCleaned\",\n",
        "                     when(col(\"Product\") == \"Product1\",\"1200\")\n",
        "                     .when(col(\"Product\") == \"Product2\",\"3600\")\n",
        "                     .otherwise(\"7500\"))\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg4gWhWrOI7A"
      },
      "source": [
        "df2.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt2W3GAPOI7C"
      },
      "source": [
        "df2 = df2.withColumn(\"PriceNum\", df2[\"PriceCleaned\"].cast(\"float\"))\r\n",
        "df2.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugjWbFPrJ30G"
      },
      "source": [
        "df3 = df2.drop(\"Price\", \"PriceCleaned\")\n",
        "df3.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E36zLEcrmqsr"
      },
      "source": [
        "df3 = df3.withColumnRenamed(\"PriceNum\",\"Price\")\r\n",
        "df3.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dat_bD3zVXFO"
      },
      "source": [
        "df3.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LWDre94S8hA"
      },
      "source": [
        "### To fill missing Latitude and Longitude values I using different interpolation techniques: mean and median imputation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlB21IJKJg2V"
      },
      "source": [
        "#clean lat column - replace null with 0 \r\n",
        "df4 = df3.withColumn(\"Lat1\", when(df3.Latitude.isNull(), 0).otherwise(df.Latitude))\r\n",
        "df4.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztz3WuPt7ffe"
      },
      "source": [
        "#### Calculate mean(Latitude) grouped by Country: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMVh8yUFYxCh"
      },
      "source": [
        "from pyspark.sql.functions import avg, col, when\r\n",
        "from pyspark.sql.window import Window\r\n",
        "w = Window().partitionBy('Country')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-GU0GtRcl8J"
      },
      "source": [
        "df5 = df4.withColumn('Latitude', when(col('Latitude').isNull(), avg(col('Lat1')).over(w)).otherwise(col('Latitude')))\r\n",
        "df5.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPmarc_J7z04"
      },
      "source": [
        "### For Longitute I apply interpolation using a median=startegy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5SIXsOkfr5b"
      },
      "source": [
        "df6 = df5.withColumn(\"Long1\", when(df5.Longitude.isNull(), 0).otherwise(df5.Longitude))\r\n",
        "df6.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qcuydb7iRjH"
      },
      "source": [
        "longCol = df6.select(\"Long1\")\r\n",
        "longCol.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNtZNIr1jDpH"
      },
      "source": [
        "#Using LinAlgebra Python library to compute median\r\n",
        "import numpy as np\r\n",
        "median = np.median(longCol.collect())\r\n",
        "median"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec3bDPLSkmf1"
      },
      "source": [
        "#replace missing Longitude values with median\r\n",
        "from pyspark.sql.functions import lit\r\n",
        "\r\n",
        "df7 = df6.withColumn('Longitude', when(col('Longitude').isNull(), lit(median)).otherwise(col('Longitude')))\r\n",
        "df7.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx7YDvOvOI7G"
      },
      "source": [
        "df7 = df6.drop(\"Lat1\", \"Long1\")\r\n",
        "df7.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YhqVDb_ogGm"
      },
      "source": [
        "# PART 3. Getting insights from our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jEdHu0LzMRc"
      },
      "source": [
        "## Q: Which Product has the highest sale?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CH0nz_VkjKm"
      },
      "source": [
        "group_data = df7.groupBy(\"Product\").agg({'Product':'count'})\r\n",
        "group_data.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4rzPw8Qs1xt"
      },
      "source": [
        "#Product1 has the highest sales\r\n",
        "group_data.agg({'count(Product)':'max'}).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDydzJ68z1ta"
      },
      "source": [
        "## Q:Which Country sells better (all products)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udq49ta9rZ6F"
      },
      "source": [
        "group_data2 = df7.groupBy(\"Country\").agg({'Product':'count'})\r\n",
        "group_data2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt_wyH24v7-c"
      },
      "source": [
        "#US sells better: 461 \r\n",
        "group_data2 = df5.groupBy(\"Country\").agg({'Product':'count'}).sort(col(\"count(Product)\").desc())\r\n",
        "group_data2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QErSXaLq2JYF"
      },
      "source": [
        "## Q:Which Country sells better per product?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFzyBbYqsYai"
      },
      "source": [
        "#Breakdown by products\r\n",
        "group_data3= df7.groupBy(\"Country\", \"Product\").agg({'Product':'count'}).sort(col(\"count(Product)\").desc())\r\n",
        "group_data3.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho5SAP980g8_"
      },
      "source": [
        "## Breakdown by Region (state) per Country"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63CjUmgTw96a"
      },
      "source": [
        "group_data4= df5.groupBy(\"Country\", \"Region\", \"Product\").agg({'Product':'count'}).orderBy(\"Country\")\r\n",
        "group_data4.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}