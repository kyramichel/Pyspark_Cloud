{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GXdXdKt7bJH"
   },
   "source": [
    "# PySpark: Cleaning data and Getting insights from It\n",
    "\n",
    "\n",
    "- There are 3 Parts: Installing Spark, Loading & Cleaning data and Getting insights from data. These are the entry points for any data analytics project! \n",
    "\n",
    "- In my earlier notebooks, I discussed in detail about installing Spark and uploading data in Colab. This notebook is focused on Data cleansing. \n",
    "\n",
    "- Data cleansing is the process of analyzing the quality of data in a data source, approving/rejecting the suggestions by the system, and making changes to the data. The quality of data is important in getting useful information from it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib7UCnGX6-z2"
   },
   "source": [
    "## PART 1. Configure PySpark environment\n",
    "\n",
    "Copy & Paste code below. \n",
    "\n",
    "Read more https://github.com/kyramichel/Pyspark_Cloud/blob/master/PySpark_GoogleColab.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdSQHcj0OI6Q"
   },
   "outputs": [],
   "source": [
    "#update the packages existing on the machine\n",
    "!apt-get update\n",
    "\n",
    "#install java \n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "\n",
    "#install spark: get the file\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz\n",
    "    \n",
    "#unzip the file\n",
    "!tar xf spark-2.4.1-bin-hadoop2.7.tgz\n",
    "\n",
    "#set up the ennvironmental variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.2-bin-hadoop2.7\"\n",
    "\n",
    "#install finspark  \n",
    "!pip install -q findspark\n",
    "\n",
    "#importing findspark adds pyspark to the system path, so that next time you can import pyspark like any other python library\n",
    "import findspark\n",
    "findspark.init(\"/content/spark-2.4.1-bin-hadoop2.7\")\n",
    "\n",
    "import pyspark\n",
    "\n",
    "#SparkContext: the entry point of spark functionality is the interface to running a spark cluster manager\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "\n",
    "#import a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "#create a session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark\n",
    "\n",
    "#test the installation\n",
    "df0 = spark.sql(\"select 'PySpark' as Hello\")\n",
    "df0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP2pY0E03QYX"
   },
   "source": [
    "# PART 2. Upload, Load & Clean Data\n",
    "\n",
    "- To upload data, click upload, select your data file\n",
    "Read more how to get in data in Colab: https://github.com/kyramichel/Pyspark_Cloud/blob/master/DataPysparkCloudColab.ipynb\n",
    "\n",
    "\n",
    "- Load data, create a data frame df \n",
    "\n",
    "- To get insights from data we can query a data frame in Spark using both Python and SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6cOc3XiLjnN"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
    "df.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6DFTHR-Hhll"
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKphjZjHOlC-"
   },
   "outputs": [],
   "source": [
    "#Import pyspark sql functions library to clean data\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7nz-ycPOFDH"
   },
   "outputs": [],
   "source": [
    "#clean Region column - we create a new col because df is immutable\n",
    "df1 = df.withColumn(\"RegionCleaned\", when(df.Region.isNull(), 'unknown').otherwise(df.Region))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ith704sXQkfh"
   },
   "outputs": [],
   "source": [
    "df1.select(\"Region\", \"RegionCleaned\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUyNWXT1lJQC"
   },
   "outputs": [],
   "source": [
    "df1.drop(\"Region\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pjaI47zlR45"
   },
   "outputs": [],
   "source": [
    "df1 = df1.withColumnRenamed(\"RegionCleaned\",\"Region\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax2R0LrySfUU"
   },
   "outputs": [],
   "source": [
    "#Use filter to delete entire row when Country is Null  \n",
    "\n",
    "df1 = df.filter(df.Country.isNotNull())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vir-lsKp_Tfu"
   },
   "outputs": [],
   "source": [
    "df2 = df1.withColumn(\"PriceCleaned\",\n",
    "                     when(col(\"Product\") == \"Product1\",\"1200\")\n",
    "                     .when(col(\"Product\") == \"Product2\",\"3600\")\n",
    "                     .otherwise(\"7500\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg4gWhWrOI7A"
   },
   "outputs": [],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pt2W3GAPOI7C"
   },
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"PriceNum\", df2[\"PriceCleaned\"].cast(\"float\"))\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugjWbFPrJ30G"
   },
   "outputs": [],
   "source": [
    "df3 = df2.drop(\"Price\", \"PriceCleaned\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E36zLEcrmqsr"
   },
   "outputs": [],
   "source": [
    "df3 = df3.withColumnRenamed(\"PriceNum\",\"Price\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dat_bD3zVXFO"
   },
   "outputs": [],
   "source": [
    "df3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LWDre94S8hA"
   },
   "source": [
    "### To fill missing Latitude and Longitude values I using different interpolation techniques: mean and median imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlB21IJKJg2V"
   },
   "outputs": [],
   "source": [
    "#clean lat column - replace null with 0 \n",
    "df4 = df3.withColumn(\"Lat1\", when(df3.Latitude.isNull(), 0).otherwise(df.Latitude))\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ztz3WuPt7ffe"
   },
   "source": [
    "#### Calculate mean(Latitude) grouped by Country: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMVh8yUFYxCh"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, when\n",
    "from pyspark.sql.window import Window\n",
    "w = Window().partitionBy('Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-GU0GtRcl8J"
   },
   "outputs": [],
   "source": [
    "df5 = df4.withColumn('Latitude', when(col('Latitude').isNull(), avg(col('Lat1')).over(w)).otherwise(col('Latitude')))\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPmarc_J7z04"
   },
   "source": [
    "### For Longitute I apply interpolation using a median=startegy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5SIXsOkfr5b"
   },
   "outputs": [],
   "source": [
    "df6 = df5.withColumn(\"Long1\", when(df5.Longitude.isNull(), 0).otherwise(df5.Longitude))\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Qcuydb7iRjH"
   },
   "outputs": [],
   "source": [
    "longCol = df6.select(\"Long1\")\n",
    "longCol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNtZNIr1jDpH"
   },
   "outputs": [],
   "source": [
    "#Using LinAlgebra Python library to compute median\n",
    "import numpy as np\n",
    "median = np.median(longCol.collect())\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec3bDPLSkmf1"
   },
   "outputs": [],
   "source": [
    "#replace missing Longitude values with median\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df7 = df6.withColumn('Longitude', when(col('Longitude').isNull(), lit(median)).otherwise(col('Longitude')))\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wx7YDvOvOI7G"
   },
   "outputs": [],
   "source": [
    "df7 = df6.drop(\"Lat1\", \"Long1\")\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YhqVDb_ogGm"
   },
   "source": [
    "# PART 3. Getting insights from our the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jEdHu0LzMRc"
   },
   "source": [
    "## Q: Which Product has the highest sale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CH0nz_VkjKm"
   },
   "outputs": [],
   "source": [
    "group_data = df7.groupBy(\"Product\").agg({'Product':'count'})\n",
    "group_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4rzPw8Qs1xt"
   },
   "outputs": [],
   "source": [
    "#Product1 has the highest sales\n",
    "group_data.agg({'count(Product)':'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDydzJ68z1ta"
   },
   "source": [
    "## Q:Which Country sells better (all products)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udq49ta9rZ6F"
   },
   "outputs": [],
   "source": [
    "group_data2 = df7.groupBy(\"Country\").agg({'Product':'count'})\n",
    "group_data2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wt_wyH24v7-c"
   },
   "outputs": [],
   "source": [
    "#US sells better: 461 \n",
    "group_data2 = df5.groupBy(\"Country\").agg({'Product':'count'}).sort(col(\"count(Product)\").desc())\n",
    "group_data2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QErSXaLq2JYF"
   },
   "source": [
    "## Q:Which Country sells better per product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFzyBbYqsYai"
   },
   "outputs": [],
   "source": [
    "#Breakdown by products\n",
    "group_data3= df7.groupBy(\"Country\", \"Product\").agg({'Product':'count'}).sort(col(\"count(Product)\").desc())\n",
    "group_data3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho5SAP980g8_"
   },
   "source": [
    "## Breakdown by Region (state) per Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63CjUmgTw96a"
   },
   "outputs": [],
   "source": [
    "group_data4= df5.groupBy(\"Country\", \"Region\", \"Product\").agg({'Product':'count'}).orderBy(\"Country\")\n",
    "group_data4.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pyspark_Cloud v2.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
